{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DQNP.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#DQNPrioritized experience replay"],"metadata":{"id":"uLh01fa98nLm"}},{"cell_type":"markdown","source":["## libraries and data"],"metadata":{"id":"UHrDDpZB8qO-"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aXHw7v1flSz3","executionInfo":{"status":"ok","timestamp":1647965420889,"user_tz":-480,"elapsed":2614,"user":{"displayName":"Hao Zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiOXQvve80GV-abIn9JNc9h5OogAaUQk_qJ5ZYdlQ=s64","userId":"05821975700436533378"}},"outputId":"735dfec7-22c6-41ca-ad0f-74fb4bce6b57"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","import sys, os\n","\n","#mount google drive\n","drive.mount('/content/drive', force_remount=False)"]},{"cell_type":"code","source":["!pip install gym pyvirtualdisplay > /dev/null 2>&1\n","!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n","!pip install gym > /dev/null 2>&1\n","!pip install pyglet > /dev/null 2>&1"],"metadata":{"id":"iMVOQgq6odD7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["os.chdir(\"/content/drive/My Drive/CS5446 Project\")"],"metadata":{"id":"TvTPrkpklvwp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python -m atari_py.import_roms Roms"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wZgVb4PEqmP-","executionInfo":{"status":"ok","timestamp":1647965433810,"user_tz":-480,"elapsed":1131,"user":{"displayName":"Hao Zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiOXQvve80GV-abIn9JNc9h5OogAaUQk_qJ5ZYdlQ=s64","userId":"05821975700436533378"}},"outputId":"d264a3ed-3999-43c1-98c6-f9a0f21f2507"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["copying adventure.bin from HC ROMS/BY ALPHABET (PAL)/A-G/Adventure (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/adventure.bin\n","copying air_raid.bin from HC ROMS/BY ALPHABET (PAL)/A-G/Air Raid (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/air_raid.bin\n","copying alien.bin from HC ROMS/BY ALPHABET (PAL)/A-G/REMAINING NTSC ORIGINALS/Alien.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/alien.bin\n","copying crazy_climber.bin from HC ROMS/BY ALPHABET (PAL)/A-G/REMAINING NTSC ORIGINALS/Crazy Climber.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/crazy_climber.bin\n","copying elevator_action.bin from HC ROMS/BY ALPHABET (PAL)/A-G/REMAINING NTSC ORIGINALS/Elevator Action (Prototype).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/elevator_action.bin\n","copying gravitar.bin from HC ROMS/BY ALPHABET (PAL)/A-G/REMAINING NTSC ORIGINALS/Gravitar.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/gravitar.bin\n","copying keystone_kapers.bin from HC ROMS/BY ALPHABET (PAL)/H-R/Keystone Kapers (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/keystone_kapers.bin\n","copying king_kong.bin from HC ROMS/BY ALPHABET (PAL)/H-R/King Kong (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/king_kong.bin\n","copying laser_gates.bin from HC ROMS/BY ALPHABET (PAL)/H-R/Laser Gates (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/laser_gates.bin\n","copying mr_do.bin from HC ROMS/BY ALPHABET (PAL)/H-R/Mr. Do! (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/mr_do.bin\n","copying pacman.bin from HC ROMS/BY ALPHABET (PAL)/H-R/Pac-Man (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pacman.bin\n","copying jamesbond.bin from HC ROMS/BY ALPHABET (PAL)/H-R/REMAINING NTSC ORIGINALS/James Bond 007.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/jamesbond.bin\n","copying koolaid.bin from HC ROMS/BY ALPHABET (PAL)/H-R/REMAINING NTSC ORIGINALS/Kool-Aid Man.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/koolaid.bin\n","copying krull.bin from HC ROMS/BY ALPHABET (PAL)/H-R/REMAINING NTSC ORIGINALS/Krull.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/krull.bin\n","copying montezuma_revenge.bin from HC ROMS/BY ALPHABET (PAL)/H-R/REMAINING NTSC ORIGINALS/Montezuma's Revenge - Featuring Panama Joe.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/montezuma_revenge.bin\n","copying star_gunner.bin from HC ROMS/BY ALPHABET (PAL)/S-Z/REMAINING NTSC ORIGINALS/Stargunner.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/star_gunner.bin\n","copying time_pilot.bin from HC ROMS/BY ALPHABET (PAL)/S-Z/REMAINING NTSC ORIGINALS/Time Pilot.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/time_pilot.bin\n","copying up_n_down.bin from HC ROMS/BY ALPHABET (PAL)/S-Z/REMAINING NTSC ORIGINALS/Up 'n Down.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/up_n_down.bin\n","copying sir_lancelot.bin from HC ROMS/BY ALPHABET (PAL)/S-Z/Sir Lancelot (PAL).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/sir_lancelot.bin\n","copying amidar.bin from HC ROMS/BY ALPHABET/A-G/Amidar.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/amidar.bin\n","copying asteroids.bin from HC ROMS/BY ALPHABET/A-G/Asteroids [no copyright].bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/asteroids.bin\n","copying atlantis.bin from HC ROMS/BY ALPHABET/A-G/Atlantis.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/atlantis.bin\n","copying bank_heist.bin from HC ROMS/BY ALPHABET/A-G/Bank Heist.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/bank_heist.bin\n","copying battle_zone.bin from HC ROMS/BY ALPHABET/A-G/Battlezone.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/battle_zone.bin\n","copying beam_rider.bin from HC ROMS/BY ALPHABET/A-G/Beamrider.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/beam_rider.bin\n","copying berzerk.bin from HC ROMS/BY ALPHABET/A-G/Berzerk.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/berzerk.bin\n","copying bowling.bin from HC ROMS/BY ALPHABET/A-G/Bowling.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/bowling.bin\n","copying boxing.bin from HC ROMS/BY ALPHABET/A-G/Boxing.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/boxing.bin\n","copying breakout.bin from HC ROMS/BY ALPHABET/A-G/Breakout - Breakaway IV.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/breakout.bin\n","copying carnival.bin from HC ROMS/BY ALPHABET/A-G/Carnival.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/carnival.bin\n","copying centipede.bin from HC ROMS/BY ALPHABET/A-G/Centipede.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/centipede.bin\n","copying chopper_command.bin from HC ROMS/BY ALPHABET/A-G/Chopper Command.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/chopper_command.bin\n","copying defender.bin from HC ROMS/BY ALPHABET/A-G/Defender.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/defender.bin\n","copying demon_attack.bin from HC ROMS/BY ALPHABET/A-G/Demon Attack.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/demon_attack.bin\n","copying donkey_kong.bin from HC ROMS/BY ALPHABET/A-G/Donkey Kong.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/donkey_kong.bin\n","copying double_dunk.bin from HC ROMS/BY ALPHABET/A-G/Double Dunk.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/double_dunk.bin\n","copying enduro.bin from HC ROMS/BY ALPHABET/A-G/Enduro.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/enduro.bin\n","copying fishing_derby.bin from HC ROMS/BY ALPHABET/A-G/Fishing Derby.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/fishing_derby.bin\n","copying freeway.bin from HC ROMS/BY ALPHABET/A-G/Freeway.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/freeway.bin\n","copying frogger.bin from HC ROMS/BY ALPHABET/A-G/Frogger.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/frogger.bin\n","copying frostbite.bin from HC ROMS/BY ALPHABET/A-G/Frostbite.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/frostbite.bin\n","copying galaxian.bin from HC ROMS/BY ALPHABET/A-G/Galaxian.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/galaxian.bin\n","copying gopher.bin from HC ROMS/BY ALPHABET/A-G/Gopher.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/gopher.bin\n","copying hero.bin from HC ROMS/BY ALPHABET/H-R/H.E.R.O..bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/hero.bin\n","copying ice_hockey.bin from HC ROMS/BY ALPHABET/H-R/Ice Hockey.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/ice_hockey.bin\n","copying journey_escape.bin from HC ROMS/BY ALPHABET/H-R/Journey Escape.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/journey_escape.bin\n","copying kaboom.bin from HC ROMS/BY ALPHABET/H-R/Kaboom!.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kaboom.bin\n","copying kangaroo.bin from HC ROMS/BY ALPHABET/H-R/Kangaroo.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kangaroo.bin\n","copying kung_fu_master.bin from HC ROMS/BY ALPHABET/H-R/Kung-Fu Master.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/kung_fu_master.bin\n","copying lost_luggage.bin from HC ROMS/BY ALPHABET/H-R/Lost Luggage [no opening scene].bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/lost_luggage.bin\n","copying ms_pacman.bin from HC ROMS/BY ALPHABET/H-R/Ms. Pac-Man.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/ms_pacman.bin\n","copying name_this_game.bin from HC ROMS/BY ALPHABET/H-R/Name This Game.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/name_this_game.bin\n","copying phoenix.bin from HC ROMS/BY ALPHABET/H-R/Phoenix.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/phoenix.bin\n","copying pitfall.bin from HC ROMS/BY ALPHABET/H-R/Pitfall! - Pitfall Harry's Jungle Adventure.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pitfall.bin\n","copying pooyan.bin from HC ROMS/BY ALPHABET/H-R/Pooyan.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pooyan.bin\n","copying private_eye.bin from HC ROMS/BY ALPHABET/H-R/Private Eye.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/private_eye.bin\n","copying qbert.bin from HC ROMS/BY ALPHABET/H-R/Q-bert.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/qbert.bin\n","copying riverraid.bin from HC ROMS/BY ALPHABET/H-R/River Raid.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/riverraid.bin\n","copying road_runner.bin from patched version of HC ROMS/BY ALPHABET/H-R/Road Runner.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/road_runner.bin\n","copying robotank.bin from HC ROMS/BY ALPHABET/H-R/Robot Tank.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/robotank.bin\n","copying seaquest.bin from HC ROMS/BY ALPHABET/S-Z/Seaquest.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/seaquest.bin\n","copying skiing.bin from HC ROMS/BY ALPHABET/S-Z/Skiing.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/skiing.bin\n","copying solaris.bin from HC ROMS/BY ALPHABET/S-Z/Solaris.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/solaris.bin\n","copying space_invaders.bin from HC ROMS/BY ALPHABET/S-Z/Space Invaders.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/space_invaders.bin\n","copying surround.bin from HC ROMS/BY ALPHABET/S-Z/Surround - Chase.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/surround.bin\n","copying tennis.bin from HC ROMS/BY ALPHABET/S-Z/Tennis.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/tennis.bin\n","copying trondead.bin from HC ROMS/BY ALPHABET/S-Z/TRON - Deadly Discs.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/trondead.bin\n","copying tutankham.bin from HC ROMS/BY ALPHABET/S-Z/Tutankham.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/tutankham.bin\n","copying venture.bin from HC ROMS/BY ALPHABET/S-Z/Venture.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/venture.bin\n","copying pong.bin from HC ROMS/BY ALPHABET/S-Z/Video Olympics - Pong Sports.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/pong.bin\n","copying video_pinball.bin from HC ROMS/BY ALPHABET/S-Z/Video Pinball - Arcade Pinball.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/video_pinball.bin\n","copying wizard_of_wor.bin from HC ROMS/BY ALPHABET/S-Z/Wizard of Wor.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/wizard_of_wor.bin\n","copying yars_revenge.bin from HC ROMS/BY ALPHABET/S-Z/Yars' Revenge.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/yars_revenge.bin\n","copying zaxxon.bin from HC ROMS/BY ALPHABET/S-Z/Zaxxon.bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/zaxxon.bin\n","copying assault.bin from HC ROMS/NTSC VERSIONS OF PAL ORIGINALS/Assault (AKA Sky Alien) (1983) (Bomb - Onbase) (CA281).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/assault.bin\n","copying asterix.bin from ROMS/Asterix (AKA Taz) (07-27-1983) (Atari, Jerome Domurat, Steve Woita) (CX2696) (Prototype).bin to /usr/local/lib/python3.7/dist-packages/atari_py/atari_roms/asterix.bin\n"]}]},{"cell_type":"markdown","source":["## Algorithm"],"metadata":{"id":"wYeVlvm881op"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.autograd as autograd \n","import torch.nn.functional as F\n","\n","import gym\n","from gym.wrappers import Monitor\n","import numpy as np\n","import glob\n","import io\n","import base64\n","from IPython.display import HTML\n","from pyvirtualdisplay import Display\n","from IPython import display as ipythondisplay\n","\n","import math, random"],"metadata":{"id":"F3B9ndT-9BNM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Environment and parameter"],"metadata":{"id":"gu_qMFQG9yWa"}},{"cell_type":"code","source":["seed = 2022\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","torch.cuda.manual_seed_all(seed)\n","\n","env_id = \"CartPole-v0\"\n","env = gym.make(env_id)\n","\n","epsilon_start = 1.0\n","epsilon_final = 0.01\n","epsilon_decay = 500\n","epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * frame_idx / epsilon_decay)\n","\n","num_frames = 10000\n","batch_size = 32\n","gamma      = 0.99"],"metadata":{"id":"n061d1dk92nq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Replay Buffer"],"metadata":{"id":"qTL7cTvi9fJt"}},{"cell_type":"code","source":["\n","from collections import deque,namedtuple\n","\n","# Standard DQN\n","# class ReplayBuffer(object):\n","#     def __init__(self, capacity):\n","#         self.buffer = deque(maxlen=capacity)\n","    \n","#     def push(self, state, action, reward, next_state, done):\n","#         state      = np.expand_dims(state, 0)\n","#         next_state = np.expand_dims(next_state, 0)\n","            \n","#         self.buffer.append((state, action, reward, next_state, done))\n","    \n","#     def sample(self, batch_size):\n","#         state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n","#         return np.concatenate(state), action, reward, np.concatenate(next_state), done\n","    \n","#     def __len__(self):\n","#         return len(self.buffer)\n","\n","\n","# Pioritized\n","\n","# update_mem_every = 100\n","# update_nn_every = 10\n","\n","# experiences_per_sampling = math.ceil(batch_size * update_mem_every / update_nn_every)\n","\n","# class ReplayBuffer(object):\n","#     def __init__(self, capacity):\n","#         self.experience_count = 0\n","\n","#         self.priorities_max = 10\n","#         self.weights_max = 5\n","\n","#         self.buffer_size = capacity\n","#         self.experience = namedtuple(\"Experience\", \n","#         field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n","#         self.data = namedtuple(\"Data\", \n","#             field_names=[\"priority\", \"probability\", \"weight\",\"index\"])\n","\n","#         indexes = []\n","#         datas = []\n","#         for i in range(capacity):\n","#             indexes.append(i)\n","#             d = self.data(0,0,0,i)\n","#             datas.append(d)\n","\n","#         self.memory = {key: self.experience for key in indexes}\n","#         self.memory_data = {key: data for key,data in zip(indexes, datas)}\n","\n","        \n","    \n","#     # def push(self, state, action, reward, next_state, done):\n","#     #     state      = np.expand_dims(state, 0)\n","#     #     next_state = np.expand_dims(next_state, 0)\n","            \n","#     #     self.buffer.append((state, action, reward, next_state, done))\n","    \n","#     # def sample(self, batch_size):\n","#     #     state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n","#     #     return np.concatenate(state), action, reward, np.concatenate(next_state), done\n","    \n","#     def __len__(self):\n","#         return len(self.buffer)\n","\n","#     def choices(self, population, weights=None, *, cum_weights=None, k=1):\n","#         \"\"\"Return a k sized list of population elements chosen with replacement.\n","#         If the relative weights or cumulative weights are not specified,\n","#         the selections are made with equal probability.\n","#         \"\"\"\n","        \n","#        ###  Removed the rest of the implementation for visibility ###\n","       \n","#         return [population[bisect(cum_weights, random() * total, 0, hi)]\n","#                 for i in _repeat(None, k)]\n","\n","#     def push(self, state, action, reward, next_state, done):\n","#         \"\"\"Add a new experience to memory.\"\"\"\n","#         self.experience_count += 1\n","#         index = self.experience_count % self.buffer_size\n","\n","#         if self.experience_count > self.buffer_size:\n","#             temp = self.memory_data[index]\n","#             self.priorities_sum_alpha -= temp.probability**self.alpha\n","#             if temp.priority == self.priorities_max:\n","#                 self.memory_data[index].priority = 0\n","#                 self.priorities_max = max(self.memory_data.items(), key=operator.itemgetter(1)).priority\n","#             if self.compute_weights:\n","#                 if temp.weight == self.weights_max:\n","#                     self.memory_data[index].weight = 0\n","#                     self.weights_max = max(self.memory_data.items(), key=operator.itemgetter(2)).weight\n","\n","#         priority = self.priorities_max\n","#         weight = self.weights_max\n","#         self.priorities_sum_alpha += priority ** self.alpha\n","#         probability = priority ** self.alpha / self.priorities_sum_alpha\n","#         e = self.experience(state, action, reward, next_state, done)\n","#         self.memory[index] = e\n","#         d = self.data(priority, probability, weight, index)\n","#         self.memory_data[index] = d\n","\n","#     def sample(self):\n","#             \"\"\"Randomly sample X batches of experiences from memory.\"\"\"\n","#             # X is the number of steps before updating memory\n","#             self.current_batch = 0\n","#             values = list(self.memory_data.values())\n","#             random_values = random.choices(self.memory_data, \n","#                                           [data.probability for data in values], \n","#                                           k=self.experiences_per_sampling)\n","#             self.sampled_batches = [random_values[i:i + batch_size] \n","#                                         for i in range(0, len(random_values), self.batch_size)]\n","\n"],"metadata":{"id":"zRVeF98o9ehW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import random\n","from collections import namedtuple, deque\n","import numpy as np\n","from numpy.random import choice\n","import torch\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","class ReplayBuffer:\n","    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n","\n","    def __init__(self, action_size, buffer_size, batch_size, experiences_per_sampling, seed, compute_weights):\n","        \"\"\"Initialize a ReplayBuffer object.\n","        Params\n","        ======\n","            action_size (int): dimension of each action\n","            buffer_size (int): maximum size of buffer\n","            experiences_per_sampling (int): number of experiences to sample during a sampling iteration\n","            batch_size (int): size of each training batch\n","            seed (int): random seed\n","        \"\"\"\n","        self.action_size = action_size\n","        self.buffer_size = buffer_size\n","        self.batch_size = batch_size\n","        self.experiences_per_sampling = experiences_per_sampling\n","        \n","        self.alpha = 0.5\n","        self.alpha_decay_rate = 0.99\n","        self.beta = 0.5\n","        self.beta_growth_rate = 1.001\n","        self.seed = random.seed(seed)\n","        self.compute_weights = compute_weights\n","        self.experience_count = 0\n","        \n","        self.experience = namedtuple(\"Experience\", \n","            field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n","        self.data = namedtuple(\"Data\", \n","            field_names=[\"priority\", \"probability\", \"weight\",\"index\"])\n","\n","        indexes = []\n","        datas = []\n","        for i in range(buffer_size):\n","            indexes.append(i)\n","            d = self.data(0,0,0,i)\n","            datas.append(d)\n","        \n","        self.memory = {key: self.experience for key in indexes}\n","        self.memory_data = {key: data for key,data in zip(indexes, datas)}\n","        self.sampled_batches = []\n","        self.current_batch = 0\n","        self.priorities_sum_alpha = 0\n","        self.priorities_max = 1\n","        self.weights_max = 1\n","    \n","    def update_priorities(self, tds, indices):\n","        for td, index in zip(tds, indices):\n","            N = min(self.experience_count, self.buffer_size)\n","\n","            updated_priority = td[0]\n","            if updated_priority > self.priorities_max:\n","                self.priorities_max = updated_priority\n","            \n","            if self.compute_weights:\n","                updated_weight = ((N * updated_priority)**(-self.beta))/self.weights_max\n","                if updated_weight > self.weights_max:\n","                    self.weights_max = updated_weight\n","            else:\n","                updated_weight = 1\n","\n","            old_priority = self.memory_data[index].priority\n","            self.priorities_sum_alpha += updated_priority**self.alpha - old_priority**self.alpha\n","            updated_probability = td[0]**self.alpha / self.priorities_sum_alpha\n","            data = self.data(updated_priority, updated_probability, updated_weight, index) \n","            self.memory_data[index] = data\n","\n","    def update_memory_sampling(self):\n","        \"\"\"Randomly sample X batches of experiences from memory.\"\"\"\n","        # X is the number of steps before updating memory\n","        self.current_batch = 0\n","        values = list(self.memory_data.values())\n","        random_values = random.choices(self.memory_data, \n","                                       [data.probability for data in values], \n","                                       k=self.experiences_per_sampling)\n","        self.sampled_batches = [random_values[i:i + self.batch_size] \n","                                    for i in range(0, len(random_values), self.batch_size)]\n","\n","    def update_parameters(self):\n","        self.alpha *= self.alpha_decay_rate\n","        self.beta *= self.beta_growth_rate\n","        if self.beta > 1:\n","            self.beta = 1\n","        N = min(self.experience_count, self.buffer_size)\n","        self.priorities_sum_alpha = 0\n","        sum_prob_before = 0\n","        for element in self.memory_data.values():\n","            sum_prob_before += element.probability\n","            self.priorities_sum_alpha += element.priority**self.alpha\n","        sum_prob_after = 0\n","        for element in self.memory_data.values():\n","            probability = element.priority**self.alpha / self.priorities_sum_alpha\n","            sum_prob_after += probability\n","            weight = 1\n","            if self.compute_weights:\n","                weight = ((N *  element.probability)**(-self.beta))/self.weights_max\n","            d = self.data(element.priority, probability, weight, element.index)\n","            self.memory_data[element.index] = d\n","        print(\"sum_prob before\", sum_prob_before)\n","        print(\"sum_prob after : \", sum_prob_after)\n","    \n","    def add(self, state, action, reward, next_state, done):\n","        \"\"\"Add a new experience to memory.\"\"\"\n","        self.experience_count += 1\n","        index = self.experience_count % self.buffer_size\n","\n","        if self.experience_count > self.buffer_size:\n","            temp = self.memory_data[index]\n","            self.priorities_sum_alpha -= temp.priority**self.alpha\n","            if temp.priority == self.priorities_max:\n","                print(temp.priority,self.priorities_max)\n","\n","                self.memory_data[index].priority = 0\n","                # self.memory_data[index].priority._replace(0)\n","\n","                self.priorities_max = max(self.memory_data.items(), key=operator.itemgetter(1)).priority\n","            if self.compute_weights:\n","                if temp.weight == self.weights_max:\n","                    self.memory_data[index].weight = 0\n","                    self.weights_max = max(self.memory_data.items(), key=operator.itemgetter(2)).weight\n","\n","        priority = self.priorities_max\n","        weight = self.weights_max\n","        self.priorities_sum_alpha += priority ** self.alpha\n","        probability = priority ** self.alpha / self.priorities_sum_alpha\n","        e = self.experience(state, action, reward, next_state, done)\n","        self.memory[index] = e\n","        d = self.data(priority, probability, weight, index)\n","        self.memory_data[index] = d\n","            \n","    def sample(self):\n","        sampled_batch = self.sampled_batches[self.current_batch]\n","        self.current_batch += 1\n","        experiences = []\n","        weights = []\n","        indices = []\n","        \n","        for data in sampled_batch:\n","            experiences.append(self.memory.get(data.index))\n","            weights.append(data.weight)\n","            indices.append(data.index)\n","\n","        states = torch.from_numpy(\n","            np.vstack([e.state for e in experiences if e is not None])).float()\n","        actions = torch.from_numpy(\n","            np.vstack([e.action for e in experiences if e is not None])).long()\n","        rewards = torch.from_numpy(\n","            np.vstack([e.reward for e in experiences if e is not None])).float()\n","        next_states = torch.from_numpy(\n","            np.vstack([e.next_state for e in experiences if e is not None])).float()\n","        dones = torch.from_numpy(\n","            np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float()\n","\n","              # states = torch.from_numpy(\n","        #     np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n","        # actions = torch.from_numpy(\n","        #     np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n","        # rewards = torch.from_numpy(\n","        #     np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n","        # next_states = torch.from_numpy(\n","        #     np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n","        # dones = torch.from_numpy(\n","        #     np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n","\n","        return (states, actions, rewards, next_states, dones, weights, indices)\n","\n","    def __len__(self):\n","        \"\"\"Return the current size of internal memory.\"\"\"\n","        return len(self.memory)"],"metadata":{"id":"LMIOZJzRD3ZD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Deep Q Network"],"metadata":{"id":"wBn05wn99-dH"}},{"cell_type":"code","source":["class DQNP(nn.Module):\n","    def __init__(self, num_inputs, num_actions):\n","        super(DQNP, self).__init__()\n","        \n","        self.layers = nn.Sequential(\n","            nn.Linear(env.observation_space.shape[0], 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, env.action_space.n)\n","        )\n","        \n","    def forward(self, x):\n","        return self.layers(x)\n","    \n","    def act(self, state, epsilon):\n","        if random.random() > epsilon:\n","            state   = torch.FloatTensor(state).unsqueeze(0).to(device)\n","            q_value = self.forward(state)\n","            action  = q_value.max(1)[1].data[0].item()\n","        else:\n","            action = random.randrange(env.action_space.n)\n","        return action"],"metadata":{"id":"EQI2jGGR-B4L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["UPDATE_NN_EVERY = 1        # how often to update the network\n","\n","# prioritized experience replay\n","BATCH_SIZE = batch_size\n","UPDATE_MEM_EVERY = 20          # how often to update the priorities\n","UPDATE_MEM_PAR_EVERY = 3000     # how often to update the hyperparameters\n","EXPERIENCES_PER_SAMPLING = math.ceil(BATCH_SIZE * UPDATE_MEM_EVERY / UPDATE_NN_EVERY)\n","\n","model = DQNP(env.observation_space.shape[0], env.action_space.n)\n","model = model.to(device)\n","    \n","optimizer = optim.Adam(model.parameters())\n","\n","# replay_buffer = ReplayBuffer(1000)\n","replay_buffer = ReplayBuffer(env.action_space.n,1000,batch_size, EXPERIENCES_PER_SAMPLING,seed,False)"],"metadata":{"id":"jvoyXuAl-EGz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Compute Loss"],"metadata":{"id":"k9B72S7ZAPiQ"}},{"cell_type":"code","source":["def compute_td_loss():\n","    state, action, reward, next_state, done, weights, indices  = replay_buffer.sample()\n","\n","    # print(state.shape,action.shape)\n","    # print(env.action_space.n)\n","    # print(EXPERIENCES_PER_SAMPLING)\n","    # print(action.shape)\n","    # print(action.unsqueeze(1))\n","\n","    state      = torch.FloatTensor(np.float32(state)).to(device)\n","    next_state = torch.FloatTensor(np.float32(next_state)).to(device)\n","    action     = torch.LongTensor(action).to(device)\n","    reward     = torch.FloatTensor(reward).to(device)\n","    done       = torch.FloatTensor(done).to(device)\n","\n","    q_values      = model(state)\n","    next_q_values = model(next_state)\n","\n","    q_value          = q_values.gather(1, action).squeeze(1)\n","    # q_value          = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n","\n","    next_q_value     = next_q_values.max(1)[0]\n","    expected_q_value = reward + gamma * next_q_value  * (1 - done)\n","    \n","    loss = (q_value - expected_q_value.data).pow(2).mean()\n","        \n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","    \n","    return loss"],"metadata":{"id":"eskyEjwVAR08"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### training"],"metadata":{"id":"n3tE0WN3NZgF"}},{"cell_type":"code","source":["\n","\n","losses = []\n","all_rewards = []\n","episode_reward = 0\n","\n","state = env.reset()\n","\n","replay_buffer.update_memory_sampling()\n","# replay_buffer.update_parameters()\n","\n","for frame_idx in range(num_frames ):\n","    epsilon = epsilon_by_frame(frame_idx+1)\n","    action = model.act(state, epsilon)\n","    \n","    next_state, reward, done, _ = env.step(action)\n","    print(frame_idx)\n","    replay_buffer.add(state, action, reward, next_state, done)\n","\n","    if frame_idx % UPDATE_MEM_PAR_EVERY == 0:\n","            replay_buffer.update_parameters()\n","    if frame_idx % UPDATE_NN_EVERY == 0:\n","        # If enough samples are available in memory, get random subset and learn\n","        # if replay_buffer.experience_count > EXPERIENCES_PER_SAMPLING:\n","        if len(replay_buffer) > batch_size:\n","            loss = compute_td_loss()\n","            losses.append(loss.item())\n","                # sampling = replay_buffer.sample()\n","            # self.learn(sampling, GAMMA)\n","    if frame_idx % UPDATE_MEM_EVERY == 0:\n","        replay_buffer.update_memory_sampling()\n","    \n","    state = next_state\n","    episode_reward += reward\n","    \n","    if done:\n","        state = env.reset()\n","        all_rewards.append(episode_reward)\n","        episode_reward = 0\n","\n","    # if len(replay_buffer) > batch_size:\n","    #     loss = compute_td_loss()\n","    #     losses.append(loss.item())\n","\n"],"metadata":{"id":"MiCjpQdkNbOA","executionInfo":{"status":"error","timestamp":1647966155486,"user_tz":-480,"elapsed":6,"user":{"displayName":"Hao Zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiOXQvve80GV-abIn9JNc9h5OogAaUQk_qJ5ZYdlQ=s64","userId":"05821975700436533378"}},"colab":{"base_uri":"https://localhost:8080/","height":460},"outputId":"257d16cc-9beb-4bb0-e2e4-6cfb1baf41dc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0\n","sum_prob before 1.0\n","sum_prob after :  1.0\n"]},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-200-069f2cc0aadf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# if replay_buffer.experience_count > EXPERIENCES_PER_SAMPLING:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_td_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0;31m# sampling = replay_buffer.sample()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-199-4bbda6dba16d>\u001b[0m in \u001b[0;36mcompute_td_loss\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompute_td_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# print(state.shape,action.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m# print(env.action_space.n)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-196-8c55135d9d33>\u001b[0m in \u001b[0;36msample\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         states = torch.from_numpy(\n\u001b[0;32m--> 152\u001b[0;31m             np.vstack([e.state for e in experiences if e is not None])).float()\n\u001b[0m\u001b[1;32m    153\u001b[0m         actions = torch.from_numpy(\n\u001b[1;32m    154\u001b[0m             np.vstack([e.action for e in experiences if e is not None])).long()\n","\u001b[0;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool."]}]},{"cell_type":"code","source":["from IPython.display import clear_output\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","plt.title('loss')\n","plt.plot(losses)\n","plt.show()\n","\n","plt.title('reward')\n","plt.plot(all_rewards)\n","plt.show()\n","\n"],"metadata":{"id":"Q7LVkOqN7CYA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Generate Video"],"metadata":{"id":"PoZouM7C9Ds4"}},{"cell_type":"code","source":["\"\"\"\n","Utility functions to enable video recording of gym environment \n","and displaying it.\n","To enable video, just do \"env = wrap_env(env)\"\"\n","\"\"\"\n","\n","def show_video():\n","    mp4list = glob.glob('video/*.mp4')\n","    if len(mp4list) > 0:\n","        mp4 = mp4list[0]\n","        video = io.open(mp4, 'r+b').read()\n","        encoded = base64.b64encode(video)\n","        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay\n","                loop controls style=\"height: 400px;\">\n","                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","             </video>'''.format(encoded.decode('ascii'))))\n","    else:\n","        print(\"Could not find video\")\n","    \n","\n","def wrap_env(env):\n","    display = Display(visible=0, size=(1400, 900))\n","    display.start()\n","    env = Monitor(env, './video/dqnp', force=True)\n","    return env"],"metadata":{"id":"fEMpWKRGlVci"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# env = wrap_env(gym.make(\"BreakoutDeterministic-v4\"))\n","env = wrap_env(gym.make(\"CartPole-v0\"))\n","\n","\n","\n","observation = env.reset()\n","\n","while True:\n","  \n","    # env.render()\n","    \n","    #your agent goes here\n","    action = model.act(torch.tensor(observation, dtype=torch.float), 0)\n","         \n","    observation, reward, done, info = env.step(action) \n","        \n","    if done: \n","        break\n","            \n","env.close()\n","show_video()"],"metadata":{"id":"L3Mwn95GqtU1"},"execution_count":null,"outputs":[]}]}