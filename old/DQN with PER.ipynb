{"cells":[{"cell_type":"markdown","metadata":{"id":"uLh01fa98nLm"},"source":["# DQN"]},{"cell_type":"markdown","metadata":{"id":"UHrDDpZB8qO-"},"source":["## libraries and data"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":29,"status":"error","timestamp":1648046306392,"user":{"displayName":"Yihang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18420492408840673244"},"user_tz":-480},"id":"TvTPrkpklvwp","colab":{"base_uri":"https://localhost:8080/","height":189},"outputId":"dbb6dd22-665d-48f5-b3c5-02dc68954acd"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-e8bbf8ebe451>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/My Drive/CS5446 Project/common\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/CS5446 Project/common'"]}],"source":["import sys, os\n","os.chdir(\"/content/drive/My Drive/CS5446 Project/common\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wZgVb4PEqmP-"},"outputs":[],"source":["!python -m atari_py.import_roms ../Roms\n","!pip install gym pyvirtualdisplay > /dev/null 2>&1\n","!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aXHw7v1flSz3"},"outputs":[],"source":["from google.colab import drive\n","\n","from copy import deepcopy\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.autograd as autograd \n","import torch.nn.functional as F\n","\n","import gym\n","import numpy as np\n","\n","import math, random\n","from wrappers import make_atari, wrap_deepmind, wrap_pytorch\n","\n","from IPython.display import HTML\n","from pyvirtualdisplay import Display\n","from IPython import display as ipythondisplay\n","from gym.wrappers import Monitor\n","\n","\n","#mount google drive\n","drive.mount('/content/drive', force_remount=False)"]},{"cell_type":"markdown","metadata":{"id":"wYeVlvm881op"},"source":["## Algorithm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F3B9ndT-9BNM"},"outputs":[],"source":[""]},{"cell_type":"code","source":["from IPython.display import clear_output\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"metadata":{"id":"WVKwL5bxu5yr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gu_qMFQG9yWa"},"source":["### Environment and parameter"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n061d1dk92nq"},"outputs":[],"source":["seed = 2022\n","# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","torch.cuda.manual_seed_all(seed)\n","\n","env_id = \"Breakout-v0\"\n","# env_id = \"BreakoutDeterministic-v4\"\n","# env_id = \"CartPole-v0\"\n","\n","# \n","env = gym.make(env_id)\n","# env = wrap_deepmind(env)\n","env = wrap_pytorch(env)\n","\n","epsilon_start = 1.0\n","epsilon_final = 0.01\n","epsilon_decay = 20000\n","epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * frame_idx / epsilon_decay)\n","\n","num_frames = 200000\n","batch_size = 32\n","gamma      = 0.99\n","\n","def set_seed(env, seed=0):\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","    env.seed(seed)\n","    env.action_space.seed(seed)\n","    np.random.seed(seed)\n","    random.seed(seed)\n","    torch.manual_seed(seed)\n","\n","\n","def device(force_cpu=False):\n","    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"Bq_j3y5h0xIh"},"source":["### display epsilon over training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CV4bmiM10uyW"},"outputs":[],"source":["plt.plot([epsilon_by_frame(i) for i in range(num_frames)])"]},{"cell_type":"markdown","metadata":{"id":"ewr1EFy00WjY"},"source":["### Deep Q Network"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j1oGpf630VQD"},"outputs":[],"source":["class CnnDQN(nn.Module):\n","    def __init__(self, input_shape, num_actions):\n","        super(CnnDQN, self).__init__()\n","        \n","        self.input_shape = input_shape\n","        self.num_actions = num_actions\n","        \n","        self.features = nn.Sequential(\n","            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n","            nn.ReLU(),\n","            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n","            nn.ReLU(),\n","            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n","            nn.ReLU()\n","        )\n","        \n","        self.fc = nn.Sequential(\n","            nn.Linear(self.feature_size(), 512),\n","            nn.ReLU(),\n","            nn.Linear(512, self.num_actions)\n","        )\n","        # self.layers = nn.Sequential(\n","        #     nn.Linear(env.observation_space.shape[0], 128),\n","        #     nn.ReLU(),\n","        #     nn.Linear(128, 128),\n","        #     nn.ReLU(),\n","        #     nn.Linear(128, env.action_space.n)\n","        # )\n","        \n","    def forward(self, x):\n","        x = self.features(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.fc(x)\n","        return x\n","    \n","    def feature_size(self):\n","        return self.features(autograd.Variable(torch.zeros(1, *self.input_shape))).view(1, -1).size(1)\n","    \n","    def act(self, state, epsilon):\n","        if random.random() > epsilon:\n","            state = torch.FloatTensor(np.float32(state)).unsqueeze(0)\n","            state = state.cuda()\n","            q_value = self.forward(state)\n","            action  = q_value.max(1)[1].data[0]\n","        else:\n","            action = random.randrange(env.action_space.n)\n","        return action"]},{"cell_type":"markdown","metadata":{"id":"qTL7cTvi9fJt"},"source":["### Replay Buffer with Priority"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zRVeF98o9ehW"},"outputs":[],"source":["#code from openai\n","#https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py\n","\n","import numpy as np\n","import random\n","\n","import operator\n","\n","\n","class SegmentTree(object):\n","    def __init__(self, capacity, operation, neutral_element):\n","        \"\"\"Build a Segment Tree data structure.\n","        https://en.wikipedia.org/wiki/Segment_tree\n","        Can be used as regular array, but with two\n","        important differences:\n","            a) setting item's value is slightly slower.\n","               It is O(lg capacity) instead of O(1).\n","            b) user has access to an efficient `reduce`\n","               operation which reduces `operation` over\n","               a contiguous subsequence of items in the\n","               array.\n","        Paramters\n","        ---------\n","        capacity: int\n","            Total size of the array - must be a power of two.\n","        operation: lambda obj, obj -> obj\n","            and operation for combining elements (eg. sum, max)\n","            must for a mathematical group together with the set of\n","            possible values for array elements.\n","        neutral_element: obj\n","            neutral element for the operation above. eg. float('-inf')\n","            for max and 0 for sum.\n","        \"\"\"\n","        assert capacity > 0 and capacity & (capacity - 1) == 0, \"capacity must be positive and a power of 2.\"\n","        self._capacity = capacity\n","        self._value = [neutral_element for _ in range(2 * capacity)]\n","        self._operation = operation\n","\n","    def _reduce_helper(self, start, end, node, node_start, node_end):\n","        if start == node_start and end == node_end:\n","            return self._value[node]\n","        mid = (node_start + node_end) // 2\n","        if end <= mid:\n","            return self._reduce_helper(start, end, 2 * node, node_start, mid)\n","        else:\n","            if mid + 1 <= start:\n","                return self._reduce_helper(start, end, 2 * node + 1, mid + 1, node_end)\n","            else:\n","                return self._operation(\n","                    self._reduce_helper(start, mid, 2 * node, node_start, mid),\n","                    self._reduce_helper(mid + 1, end, 2 * node + 1, mid + 1, node_end)\n","                )\n","\n","    def reduce(self, start=0, end=None):\n","        \"\"\"Returns result of applying `self.operation`\n","        to a contiguous subsequence of the array.\n","            self.operation(arr[start], operation(arr[start+1], operation(... arr[end])))\n","        Parameters\n","        ----------\n","        start: int\n","            beginning of the subsequence\n","        end: int\n","            end of the subsequences\n","        Returns\n","        -------\n","        reduced: obj\n","            result of reducing self.operation over the specified range of array elements.\n","        \"\"\"\n","        if end is None:\n","            end = self._capacity\n","        if end < 0:\n","            end += self._capacity\n","        end -= 1\n","        return self._reduce_helper(start, end, 1, 0, self._capacity - 1)\n","\n","    def __setitem__(self, idx, val):\n","        # index of the leaf\n","        idx += self._capacity\n","        self._value[idx] = val\n","        idx //= 2\n","        while idx >= 1:\n","            self._value[idx] = self._operation(\n","                self._value[2 * idx],\n","                self._value[2 * idx + 1]\n","            )\n","            idx //= 2\n","\n","    def __getitem__(self, idx):\n","        assert 0 <= idx < self._capacity\n","        return self._value[self._capacity + idx]\n","\n","\n","class SumSegmentTree(SegmentTree):\n","    def __init__(self, capacity):\n","        super(SumSegmentTree, self).__init__(\n","            capacity=capacity,\n","            operation=operator.add,\n","            neutral_element=0.0\n","        )\n","\n","    def sum(self, start=0, end=None):\n","        \"\"\"Returns arr[start] + ... + arr[end]\"\"\"\n","        return super(SumSegmentTree, self).reduce(start, end)\n","\n","    def find_prefixsum_idx(self, prefixsum):\n","        \"\"\"Find the highest index `i` in the array such that\n","            sum(arr[0] + arr[1] + ... + arr[i - i]) <= prefixsum\n","        if array values are probabilities, this function\n","        allows to sample indexes according to the discrete\n","        probability efficiently.\n","        Parameters\n","        ----------\n","        perfixsum: float\n","            upperbound on the sum of array prefix\n","        Returns\n","        -------\n","        idx: int\n","            highest index satisfying the prefixsum constraint\n","        \"\"\"\n","        assert 0 <= prefixsum <= self.sum() + 1e-5\n","        idx = 1\n","        while idx < self._capacity:  # while non-leaf\n","            if self._value[2 * idx] > prefixsum:\n","                idx = 2 * idx\n","            else:\n","                prefixsum -= self._value[2 * idx]\n","                idx = 2 * idx + 1\n","        return idx - self._capacity\n","\n","\n","class MinSegmentTree(SegmentTree):\n","    def __init__(self, capacity):\n","        super(MinSegmentTree, self).__init__(\n","            capacity=capacity,\n","            operation=min,\n","            neutral_element=float('inf')\n","        )\n","\n","    def min(self, start=0, end=None):\n","        \"\"\"Returns min(arr[start], ...,  arr[end])\"\"\"\n","\n","        return super(MinSegmentTree, self).reduce(start, end)\n","\n","class ReplayBuffer(object):\n","    def __init__(self, size):\n","        \"\"\"Create Replay buffer.\n","        Parameters\n","        ----------\n","        size: int\n","            Max number of transitions to store in the buffer. When the buffer\n","            overflows the old memories are dropped.\n","        \"\"\"\n","        self._storage = []\n","        self._maxsize = size\n","        self._next_idx = 0\n","\n","    def __len__(self):\n","        return len(self._storage)\n","\n","    def push(self, state, action, reward, next_state, done):\n","        data = (state, action, reward, next_state, done)\n","\n","        if self._next_idx >= len(self._storage):\n","            self._storage.append(data)\n","        else:\n","            self._storage[self._next_idx] = data\n","        self._next_idx = (self._next_idx + 1) % self._maxsize\n","\n","    def _encode_sample(self, idxes):\n","        obses_t, actions, rewards, obses_tp1, dones = [], [], [], [], []\n","        for i in idxes:\n","            data = self._storage[i]\n","            obs_t, action, reward, obs_tp1, done = data\n","            obses_t.append(np.array(obs_t, copy=False))\n","            actions.append(np.array(action, copy=False))\n","            rewards.append(reward)\n","            obses_tp1.append(np.array(obs_tp1, copy=False))\n","            dones.append(done)\n","        return np.array(obses_t), np.array(actions), np.array(rewards), np.array(obses_tp1), np.array(dones)\n","\n","    def sample(self, batch_size):\n","        \"\"\"Sample a batch of experiences.\n","        Parameters\n","        ----------\n","        batch_size: int\n","            How many transitions to sample.\n","        Returns\n","        -------\n","        obs_batch: np.array\n","            batch of observations\n","        act_batch: np.array\n","            batch of actions executed given obs_batch\n","        rew_batch: np.array\n","            rewards received as results of executing act_batch\n","        next_obs_batch: np.array\n","            next set of observations seen after executing act_batch\n","        done_mask: np.array\n","            done_mask[i] = 1 if executing act_batch[i] resulted in\n","            the end of an episode and 0 otherwise.\n","        \"\"\"\n","        idxes = [random.randint(0, len(self._storage) - 1) for _ in range(batch_size)]\n","        return self._encode_sample(idxes)\n","\n","class PrioritizedReplayBuffer(ReplayBuffer):\n","    def __init__(self, size, alpha):\n","        \"\"\"Create Prioritized Replay buffer.\n","        Parameters\n","        ----------\n","        size: int\n","            Max number of transitions to store in the buffer. When the buffer\n","            overflows the old memories are dropped.\n","        alpha: float\n","            how much prioritization is used\n","            (0 - no prioritization, 1 - full prioritization)\n","        See Also\n","        --------\n","        ReplayBuffer.__init__\n","        \"\"\"\n","        super(PrioritizedReplayBuffer, self).__init__(size)\n","        assert alpha > 0\n","        self._alpha = alpha\n","\n","        it_capacity = 1\n","        while it_capacity < size:\n","            it_capacity *= 2\n","\n","        self._it_sum = SumSegmentTree(it_capacity)\n","        self._it_min = MinSegmentTree(it_capacity)\n","        self._max_priority = 1.0\n","\n","    def push(self, *args, **kwargs):\n","        \"\"\"See ReplayBuffer.store_effect\"\"\"\n","        idx = self._next_idx\n","        super(PrioritizedReplayBuffer, self).push(*args, **kwargs)\n","        self._it_sum[idx] = self._max_priority ** self._alpha\n","        self._it_min[idx] = self._max_priority ** self._alpha\n","\n","    def _sample_proportional(self, batch_size):\n","        res = []\n","        for _ in range(batch_size):\n","            # TODO(szymon): should we ensure no repeats?\n","            mass = random.random() * self._it_sum.sum(0, len(self._storage) - 1)\n","            idx = self._it_sum.find_prefixsum_idx(mass)\n","            res.append(idx)\n","        return res\n","\n","    def sample(self, batch_size, beta):\n","        \"\"\"Sample a batch of experiences.\n","        compared to ReplayBuffer.sample\n","        it also returns importance weights and idxes\n","        of sampled experiences.\n","        Parameters\n","        ----------\n","        batch_size: int\n","            How many transitions to sample.\n","        beta: float\n","            To what degree to use importance weights\n","            (0 - no corrections, 1 - full correction)\n","        Returns\n","        -------\n","        obs_batch: np.array\n","            batch of observations\n","        act_batch: np.array\n","            batch of actions executed given obs_batch\n","        rew_batch: np.array\n","            rewards received as results of executing act_batch\n","        next_obs_batch: np.array\n","            next set of observations seen after executing act_batch\n","        done_mask: np.array\n","            done_mask[i] = 1 if executing act_batch[i] resulted in\n","            the end of an episode and 0 otherwise.\n","        weights: np.array\n","            Array of shape (batch_size,) and dtype np.float32\n","            denoting importance weight of each sampled transition\n","        idxes: np.array\n","            Array of shape (batch_size,) and dtype np.int32\n","            idexes in buffer of sampled experiences\n","        \"\"\"\n","        assert beta > 0\n","\n","        idxes = self._sample_proportional(batch_size)\n","\n","        weights = []\n","        p_min = self._it_min.min() / self._it_sum.sum()\n","        max_weight = (p_min * len(self._storage)) ** (-beta)\n","\n","        for idx in idxes:\n","            p_sample = self._it_sum[idx] / self._it_sum.sum()\n","            weight = (p_sample * len(self._storage)) ** (-beta)\n","            weights.append(weight / max_weight)\n","        weights = np.array(weights)\n","        encoded_sample = self._encode_sample(idxes)\n","        return tuple(list(encoded_sample) + [weights, idxes])\n","\n","    def update_priorities(self, idxes, priorities):\n","        \"\"\"Update priorities of sampled transitions.\n","        sets priority of transition at index idxes[i] in buffer\n","        to priorities[i].\n","        Parameters\n","        ----------\n","        idxes: [int]\n","            List of idxes of sampled transitions\n","        priorities: [float]\n","            List of updated priorities corresponding to\n","            transitions at the sampled idxes denoted by\n","            variable `idxes`.\n","        \"\"\"\n","        assert len(idxes) == len(priorities)\n","        for idx, priority in zip(idxes, priorities):\n","            assert priority > 0\n","            assert 0 <= idx < len(self._storage)\n","            self._it_sum[idx] = priority ** self._alpha\n","            self._it_min[idx] = priority ** self._alpha\n","\n","            self._max_priority = max(self._max_priority, priority)"]},{"cell_type":"markdown","source":["### Init model"],"metadata":{"id":"82zXAtUq4kRp"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"wVEFntTow1kr"},"outputs":[],"source":["model = CnnDQN(env.observation_space.shape, env.action_space.n)\n","\n","if torch.cuda.is_available():\n","    model = model.cuda()\n","    \n","optimizer = optim.Adam(model.parameters(), lr=0.002)\n","\n","print(env.observation_space.shape)\n","\n","print(type(env.observation_space))\n","\n","replay_initial = 10000\n","replay_buffer = PrioritizedReplayBuffer(1000,1)"]},{"cell_type":"markdown","metadata":{"id":"k9B72S7ZAPiQ"},"source":["### Compute Loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eskyEjwVAR08"},"outputs":[],"source":["def compute_td_loss(batch_size):\n","\n","    data = replay_buffer.sample(batch_size,1)\n","\n","    print(type(data))\n","\n","    batch, weights, idxes = replay_buffer.sample(batch_size,1)\n","    state, action, reward, next_state, done = batch\n","\n","    print(state.shape)\n","\n","    state      = torch.FloatTensor(np.float32(state)).to(device)\n","    next_state = torch.FloatTensor(np.float32(next_state)).to(device)\n","    action     = torch.LongTensor(action.cpu()).to(device)\n","    reward     = torch.FloatTensor(reward).to(device)\n","    done       = torch.FloatTensor(done).to(device)\n","\n","    q_values      = model(state)\n","    next_q_values = model(next_state)\n","\n","    q_value          = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n","    next_q_value     = next_q_values.max(1)[0]\n","    expected_q_value = reward + gamma * next_q_value  * (1 - done)\n","    \n","    loss = (q_value - expected_q_value.data).pow(2).mean()\n","\n","    replay_buffer.update_priorities(idxes,reward**2/100)\n","\n","\n","        \n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    print(loss)\n","\n","    \n","    return loss"]},{"cell_type":"markdown","metadata":{"id":"gic0yt6x1JS3"},"source":["### plot function"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eYV2OqFK1LAf"},"outputs":[],"source":["def plot(frame_idx, rewards, losses):\n","    clear_output(True)\n","    plt.figure(figsize=(20,5))\n","    plt.subplot(131)\n","    plt.title('frame %s. reward: %s' % (frame_idx, np.mean(rewards[-10:])))\n","    plt.plot(rewards)\n","    plt.subplot(132)\n","    plt.title('loss')\n","    plt.plot(losses)\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"n3tE0WN3NZgF"},"source":["### training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MiCjpQdkNbOA"},"outputs":[],"source":["losses = []\n","all_rewards = []\n","episode_reward = 0\n","\n","state = env.reset()\n","for frame_idx in range(1, num_frames + 1):\n","    epsilon = epsilon_by_frame(frame_idx)\n","    action = model.act(state, epsilon)\n","    # torch_state = torch.as_tensor(state, dtype=torch.float).to(device())\n","    # torch_state = torch.as_tensor(state, dtype=torch.float)\n","\n","    # action = model.act(state, epsilon)\n","    # action = torch.as_tensor(model.act(state, epsilon))\n","\n","    # print(state.shape)\n","    # print(action)\n","    \n","    \n","    next_state, reward, done, _ = env.step(action)\n","    \n","    replay_buffer.push(state, action, reward, next_state, done)\n","    \n","    state = next_state\n","    episode_reward += reward\n","\n","    # print(state)\n","    \n","    if done:\n","        state = env.reset()\n","        all_rewards.append(episode_reward)\n","        episode_reward = 0\n","        \n","    if  frame_idx > len(replay_buffer):\n","        loss = compute_td_loss(batch_size)\n","        print(loss)\n","        losses.append(loss.item())\n","            \n","    # if frame_idx % 1000 == 0:\n","        # plot(frame_idx, all_rewards, losses)\n","\n","\n","    if frame_idx % 1000 ==0:\n","        print(frame_idx)\n","        print(losses[-10:])\n","        # print(replay_buffer.max_priority)\n"]},{"cell_type":"code","source":["\"\"\"\n","Utility functions to enable video recording of gym environment \n","and displaying it.\n","To enable video, just do \"env = wrap_env(env)\"\"\n","\"\"\"\n","\n","\n","\n","def show_video():\n","    mp4list = glob.glob('video/*.mp4')\n","    if len(mp4list) > 0:\n","        mp4 = mp4list[0]\n","        video = io.open(mp4, 'r+b').read()\n","        encoded = base64.b64encode(video)\n","        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay\n","                loop controls style=\"height: 400px;\">\n","                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","             </video>'''.format(encoded.decode('ascii'))))\n","    else:\n","        print(\"Could not find video\")\n","    \n","\n","def wrap_env(env):\n","    display = Display(visible=0, size=(1400, 900))\n","    display.start()\n","    env = Monitor(env, './video/dqnp', force=True)\n","    return env"],"metadata":{"id":"aZgdLLnnxHoS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[""],"metadata":{"id":"3alheXFL2LHL"}},{"cell_type":"markdown","source":[""],"metadata":{"id":"IsvQ4mln2MyM"}},{"cell_type":"code","source":["# env = wrap_env(gym.make(\"BreakoutDeterministic-v4\"))\n","import glob\n","import io\n","import base64\n","env = wrap_env(gym.make(env_id))\n","\n","\n","\n","observation = env.reset()\n","\n","print(observation.shape)\n","\n","while True:\n","  \n","    # env.render()\n","    \n","    #your agent goes here\n","    with torch.no_grad():\n","\n","      action = model.eval(observation,0.2)\n","         \n","      observation, reward, done, info = env.step(action) \n","        \n","    if done: \n","        break\n","            \n","env.close()\n","show_video()"],"metadata":{"id":"BivJN58r2NUs","executionInfo":{"status":"error","timestamp":1648047798273,"user_tz":-480,"elapsed":437,"user":{"displayName":"Hao Zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiOXQvve80GV-abIn9JNc9h5OogAaUQk_qJ5ZYdlQ=s64","userId":"05821975700436533378"}},"outputId":"f3ed8098-015b-49b9-e322-519f15713a5a","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-ac8dc582ce2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbase64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrap_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'wrap_env' is not defined"]}]}],"metadata":{"colab":{"collapsed_sections":[],"name":"DQN with PER.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}